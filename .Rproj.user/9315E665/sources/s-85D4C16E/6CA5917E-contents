---
title: "Decision Trees"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    css: ../../../styles.css
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
library(tidyverse)
```

# Learning objectives: 

<br> 

* Understand what a decision tree is
* Work through creating a decision tree in `R`


This lesson follows on from yesterday, where we were predicting the probability of outcomes in a certain binary class, which we noted comes broadly under the 'classification algorithm' heading.

Now we're going to learn about another classification algorithm: the **decision tree**.

<br>

# What is a decision tree?

<br> 
You may have come across a decision tree before in some form in the past, perhaps beyond the context of data or machine learning. The basic idea is that a decision tree is a map of the possible outcomes of a series of related choices. Often businesses make use of analogous constructions under titles such as 'decision flowcharts', or 'process flowcharts'.  These handy tools have also inspired wide variety of algorithms in Machine Learning both classification and regression. 
<br> 

For example, consider you would like to go out for a nice bike ride. Your decision will depend upon various factors such as time, weather, temperature, etc. We call these factors the "features" which will influence our decision. Let's focus on weather. If you recorded all the factors relating to weather conditions alongside the decision you took, you could get a table something like this.

<br>

| Day       | Outlook  | Temperature | Humidity | Wind   | Go for a bike ride? |
|-----------|----------|-------------|----------|--------|---------------------|
| Monday    | Sunny    | Mild        | Low      | Strong | No                  |
| Tuesday   | Sunny    | Mild        | Low      | Weak   | Yes                 |
| Wednesday | Sunny    | Warm        | High     | Weak   | Yes                 |
| Thursday  | Rain     | Warm        | Normal   | Strong | No                  |
| Friday    | Rain     | Warm        | Normal   | Weak   | Yes                 |

<br>

You could express this in the form of a decision tree, which might look something like this:

<br>

```{r, echo=FALSE, out.width = '80%'}
knitr::include_graphics("images/simple_tree.png")
```

<br> 

Going forward, you (or others) could use this tree to make decisions based on data you've collected about whether or not to go for a bike ride. 

# How do they work?

Decision trees can be used for linear and non-linear data, as they do not use a linear classifier or regression. This means the performance of the algorithm is independent of the linear nature of the data. They are built by splitting our data using different features, and evaluating numerical metrics. You can use a variety of metrics to do so: Entropy, Information Gain, Gini Index Coefficients, or some metrics we've learnt about previously (residual mean square error, mean square error) if using regression trees. 

Decision trees take their name from their appearance: they resemble an upside-down tree, with the root at the top and branches splitting off as we move downwards. They are constructed of **nodes and branches**. 

```{r, echo = FALSE, out.width = '80%'}
knitr::include_graphics("images/nodes.png")
```

<br> 

The **root node** is the node that starts your graph. It starts with the variable that best splits the data. The **intermediate nodes** are the nodes where variables are evaluated to generate predictions. These variables can be either categorical or continuous; we can ask questions such as "Is that dog a certain kind of dog or not) as well as ("What would the price of a house be if I had X features?". On each step of the decision tree, we try and figure out a condition on the features we have which separates our classes the best. At the bottom of the tree are the *leaf nodes**, with a probability assigned to each that represents the probability that a given data point will satisfy the correct combination of predicates to reach that leaf, or representing the various classes in which the data can be classified (in our example above, the two classes are Yes and No).  

The logic of this process can be summed up by:

> At each node, your algorithm will ask: What feature will allow me to split the observations at hand in a way that the resulting groups are as different from each other as possible (and the members of each resulting subgroup are as similar to each other as possible)?


The tree stops "growing" (for lack of a better word) once all of the other features of the data have been evaluated, and had their resulting metric calculated. The feature combination that gives the best results will be the final evaluation. You can in fact set different conditions to stop a decision tree: for example, you can set the maximum depth of tree, minimum data in each node, or a minumum reduction in the error/mininum improvement in the metric.   

The result of this process is that for each node, a list of variables, each with different metrics calculated are compared against a threshold value. The algorithm then picks the variable/threshold combination that gives you the best result for your metric. For example, this might be the highest reduction in error combined with the biggest increase in your metric. This will of course vary depending on whether you are testing categorical or numerical data, and will vary with whatever measure you are using. 

Predicting a category or numerical target value is pretty easy using decision trees. That is one of the main advantages of them: they're relatively intuitive to understand, and to interpret them all you have to do is start at the root node, look at the value of that feature (and the feature it is based off of) and depending on that, make an inference. This process is repeated until you reach a leaf. When you reach the leaf, you either read off the probability of your data belonging to that class (for classification problems, e.g. Yes/No, True/False, Black/White, etc.) or you read off the predicted numerical value (for regression trees).   

The metrics themselves are more advanced topics, and [you can read more about them here if you are interested](https://towardsdatascience.com/a-dive-into-decision-trees-a128923c9298). For now, all you need to know is that your metrics represent a measure of error, which is what we are trying to reduce using our algorithm. 

<br>

# Creating Decision trees in R
<br>

To build our decision tree, we're going to revisit the Game of Thrones dataset from earlier in the course. **We're going to use our dataset to build a decision tree predicting whether a character will die by the end of the (TV) series.** 

We'll use the following packages to help with building decision trees. Let's load these and the data now. 
<br>

```{r, message = FALSE}
library(rpart)
library(rpart.plot)
library(dplyr)
```


```{r, warning=FALSE, message = FALSE}
thrones <- read_csv("data/character_data_S01-S08.csv")
```

```{r}
head(thrones)
```

<br>

## Data cleaning 

Decision trees are slightly easier to prepare in one aspect: it's not necessary to scale the data. That's because each node of a tree partitions data by performing a Boolean operation on a feature (for example, in the `thrones` data, is `intro_time_sec >= 60`?), and this process is unaffected by the relative scale of features. 

We do still need to remove any `NA`s from the dataset, and perform any necessary variable reduction. We may also need to perform variable engineering, ensuring that all our columns are either `numeric` or `factor` type, indicating which `numeric` variables are actually categorical etc.

As a reminder, we're going to use our dataset to **build a decision tree predicting whether a character will die by the end of the (TV) series**. As such: 

* there are a few columns directly connected to character death (we'll remove these)
* there are a few which are totally redundant or irrelevant (e.g. `name`: we'll also remove these)

Then we'll replace the numeric values with strings (using information from the data dictionary within the classnotes folder) so that the factor levels have an easily interpretable meaning. Finally we remove any `NA` values. To save time, we have done this below:

<br>

```{r}
#### Instructor: SLACK THIS OUT 
library(tidyverse)

clean_thrones<- thrones %>% 
# Only keep variables of interest
  select(c(sex, religion, occupation, social_status, allegiance_last, allegiance_switched, dth_flag, featured_episode_count, prominence)) %>% 
# Convert to factor level
	mutate(sex = factor(sex, levels = c(1, 2, 9), labels = c("Male", "Female", "Unknown")),
	religion = factor(religion, levels = c(0, 1, 3, 4, 5, 6, 7, 9), labels = c("Great Stallion", "Lord of Light", "Faith of the Seven", "Old Gods", "Drowned God", "Many Faced God", "Other", "Unknown")),
	occupation = factor(occupation, levels = c(1, 2, 9), labels = c("Silk-collar", "Leather-collar", "Unknown")),
	social_status = factor(social_status, levels = c(1, 2, 9), labels = c("Highborn", "Lowborn", "Unknown")),
	allegiance_last = factor(allegiance_last, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9), labels = c("Stark", "Targaryen", "Night's Watch", "Lannister", "Greyjoy", "Bolton", "Frey", "Other", "Unknown")),
	allegiance_switched = factor(allegiance_switched, levels = c(1, 2, 9), labels = c("No", "Yes", "Unknown")),
	dth_flag = factor(dth_flag, levels = c(0, 1), labels = c("Survived", "Died"))) %>%
# Remove NAs 
 na.omit()
```

<br> 

```{r}
glimpse(clean_thrones)
```

<br> 

As we can see from the output of `glimpse()`, we now have 345 outcome with 9 variables.

## Train/test split
<br>

The next step is to create a training and testing data set. Let's have 80% of our data going into the training set, and the remaining 20% go in our test set.  

First, let's shuffle the data, which helps to ensure we are selecting random training and test sets. In this particular instance, we are going to do something called *set the seed*. 


```{r}
# set the random seed number: ONLY INSTRUCTORS
set.seed(19)
```
<br>

The main point of using the seed is to be able to reproduce a particular sequence of 'random' numbers. Generally speaking, if you don't need to be able to do that, you wouldn't set the seed. The seed number itself carries no inherent meaning except it's a way of telling the random number generator 'where to start'. You might think of it a bit like the relationship between a PIN number and your bank account. The PIN is associated with a long string of numbers (your account number), but it's not inherently an interpretable quantity (there is an interpretation, but in setting it, you ignore that).  
In short, setting the seed of R's random number generator is useful when running simulations so that the values can be reproduced. **You should not do this when creating real models - we are doing it here for teaching purposes.**  

<br>

Now we have the starting point for our random number generator, we can partition the data.  

```{r}
# get how many rows we have in total to work out the percentage
n_data <- nrow(thrones)

# create a test sample index
test_index <- sample(1:n_data, size = n_data*0.2)

# create test set
thrones_test  <- slice(clean_thrones, test_index)

# create training set
thrones_train <- slice(clean_thrones, -test_index)
```

<br>

We can check that our test and training sets have similar proportions of deceased characters. The package `janitor` has a handy function `tabyl()` for calculating tables (e.g. frquency, cross etc.). See [here](https://cran.r-project.org/web/packages/janitor/vignettes/tabyls.html) for the vignette for `tabyl()`. 

<br>

```{r}
thrones_test %>%
 janitor::tabyl(dth_flag)
```
```{r}
thrones_train %>%
 janitor::tabyl(dth_flag)
```
<br>

It's very unlikely that we'll get an exact match, but in general the gap between the two will be smaller with larger datasets. For now, this seems like a pretty even split.   


## Build tree model based on training dataset 

<br> 

Next, we build the tree using the `rpart` library. `rpart` stands for `recursive partioning and regression trees`. 
First, we pass it the target variable (`dth_flag`) and the dataset to use (`thrones_train`). The third argument represents the type of variable we are looking for: `class` is used for classifying categorical variables while the alternative `anova` is used for a continuous target variable. We then plot the tree with `rpart.plot`.

<br>

```{r}
thrones_fit <- rpart(
  formula = dth_flag ~ ., 
  data = thrones_train, 
  method = 'class'
)

rpart.plot(thrones_fit, 
           yesno = 2, 
           fallen.leaves = TRUE, 
           faclen = 6, 
           digits = 4)
```

We have a tree! Now, what on earth does this tell us? 

To start with, the variables it has picked are the ones that are deemed most informative for predicting whether someone will die (or not).  The rest have been discarded from our model. In our case it has picked `prominence`, `religion`, `allegiance_last`, and `featured_episode_count`.    

Next, each node has three pieces of information:

* The predicted result for a data point at the node (**Survived** or **Died** in this example) is on the top line.  For the root node, this means that if you look at all the data together, the most likely result is **`died`**. Cheery!
* The second line contains probability of a **`died`** result expressed as a decimal. So for example, if we look at our root node again, the chance of dying - all features considered - is 0.58. That's quite high! 
* The third line is the percentage of data points which pass through this node. This will always be 100% at the root and the leaf nodes should always total 100%. The percentage on a given node should always be the sum of the percentages on its children.

If we start to look down the tree from the root node, you will see the "branches". For example, the root node leads to a classification category related to `prominence`. We can see that if our character's `promience` is less than 0.48, then they have a 0.18 chance of dying (the blue intermediate node on the left) and if their `prominence` is greater than 0.48, they have a 0.79 chance of dying (the green intermediate node on the right). The colouring of the nodes relates to this value, with dark blue nodes representing low probability and dark green high probability. This colouring can help figure out and decode the first line in each node. Our blue node relating to prominence has the predicted result **survived** on it. This can be confusing at first, but read it this way: the predicted result is **survived**, because you have a low probability (0.18) of dying, if your prominence is less than 0.48. You can continue this logic down the whole tree.    

Rather than probabilities, you may prefer to see node with counts of characters who `Survived` and `Died`. We can do something like this (manipulating the arguments to `rpart.plot()`)

<br> 

```{r}
rpart.plot(thrones_fit, 
           yesno = 2, 
           fallen.leaves = TRUE, 
           faclen = 6, 
           digits = 4, 
           type = 4, 
           extra = 101)
```

So, for example, in the root node, it predicts `Died` overall, as a minority of 115 characters `Survived`, while a majority of 161 characters `Died`.

<br>
<blockquote class='task'>

Have a look at the two decision tree plots shown above. Look at the leaf node in the bottom left (a terminal or leaf node is one from which no further branches emerge). In the first plot, the probability of `Died` is 0.1220, while the second plot tells us that 72 characters `Survived` and 10 `Died`. 

How are these numbers from the two plots related mathematically?

<details>
<summary>**Solution**</summary>
$$\textrm{prob}(\textrm{died}) = \frac{\textrm{no. who died}}{\textrm{total no.}} = \frac{10}{10+72} =  0.1220$$
</details>
</blockquote>
<br>

<br>
<div class='emphasis'>
**Decision trees and conditional probabilities**

It may help to think of decision trees as showing **conditional probabilities**. Let's look again at the bottom left leaf node in the plots above, and follow the path from the root node down to it:

* we start at the root node, where there are $115 + 161 = 276$ characters
* we then filter those characters down to just those whose `prominence < 0.4812`, yielding $77 + 17 = 94$ characters
* finally we then filter those characters down to just those whose `religion = Lord of Light, Drowned God, Many Faced God, Other or Unknown`, giving us $72 + 10 = 82$ characters.
* **Of those 82 characters**, we find $\textrm{prob}(\textrm{died}) = \frac{10}{82} = 0.1220$ as above. We can see this is just the $\textrm{prob}(\textrm{died})$ **given** the filtering conditions applied in the earlier steps. In other words, this is a **conditional probability** depending upon the prior conditions applied in the path from the root node

</div>
<br>


We can see the rules it has used to make the tree if we type the following:

```{r}
rpart.rules(thrones_fit, cover = TRUE)
```
<br>

In the printing output of the rules, the left column gives the `Died` probability. The rules are sorted in this column. The rightmost column gives the percentage of observations in each rule (printed because we used the optional `cover = TRUE` argument). So for example, if our first rule stated:

<br>
<center>
0.12 when prominence < 0.48 & religion is Lord of Light or Drowned God or Many Faced God or Other or Unknown, cover = 30%
</center>
<br> 

this would mean that people who had a prominence score less than 0.48 and whose religion was one of Lord of Light, Drowned God, Many Faced God, Other or Unknown have a probability 0.12 of dying, and this combination of features comprises 30% of the dataset 

<br>
<div class="emphasis">
Note that the leaf node and parent percentages _may_ not sum to 100% exactly because the percentages are rounded to the nearest whole number. This is not an error!
</div>
<br>

If we keep re-running our algorithm without setting the random number seed, we'll get a _slightly_ different tree every time. That's a consequence of randomly generating our training set, but one that we definitely want: a manually selected dataset would be open to all sorts of user bias!

<br>


## Use trained model to create predictions on test dataset 

Now we have a tree, we can use it to make predictions. We can use the `add_predictions()` function from `modelr` to add a prediction for every observation in our dataset.
 
```{r, message = FALSE}
library(modelr)

# add the predictions
thrones_test_pred <- thrones_test %>%
  add_predictions(thrones_fit, type = 'class')
```

<br>
Now we can look at our predictions. For the sake of keeping the variables reduced, let's choose the ones that our decision tree model showed as most informative.
<br>

```{r}
# look at the variables 
thrones_test_pred %>%
  select(prominence, religion, allegiance_last, featured_episode_count, dth_flag, pred)
```

Here we can have a look and see at a descriptive level if we think our model is predicting well. What do you think?

<br>

## Checking model performance   
<br>

You can then also check predictive performance by creating a **confusion matrix**, which you saw yesterday for logistic regression (another classification algorithm). Here we use the function `conf_mat()` from the `yardstick` package (more on this package [here](https://tidymodels.github.io/yardstick/)). 

The values on the left are the predicted values (i.e. column `pred`) while the values at the top represent the actual values (i.e. column `dth_flag`). We tell `conf_mat()` which variable is the true value and prediected value using the arguments `truth` and `estimate`:

```{r, message = FALSE}
library(yardstick)

conf_mat <- thrones_test_pred %>%
              conf_mat(truth = dth_flag, estimate = pred)

conf_mat
```
<br>

The main diagonal represents correctly-predicted values, with the top right values showing false positives and the bottom left being false negatives. The more accurate the decision tree, the higher the main diagonal values will be. We can calculate that accuracy with a fairly simple calculation (summing the main diagional and diving by the total). The result represents the probability of our prediction being correct. We can use the function `accuracy()` from `yardstick` to calculate this. 
<br>

```{r}
accuracy <- thrones_test_pred %>%
 accuracy(truth = dth_flag, estimate = pred)

accuracy 
```

The `.estimate` column in the output shows you the probability you have of correctly predicting whether a character in the test set died or not in the Game of Thrones series. We can also calculate the **sensitivity** (AKA true positive rate) and **specificity** (AKA true negative rate) using other `yardstick` functions. You also saw these metrics yesterday when we discussed classifier performance more generally.

```{r}
thrones_test_pred %>%
  sensitivity(truth = dth_flag, estimate = pred)

thrones_test_pred %>%
  specificity(truth = dth_flag, estimate = pred)
```


<br>

**Note**: you have also already used the `caret` packag. To calculate the confusion matrix and accuracy using `caret` we could use the function `confusionMatrix()` (which also returns other metrics)

```{r, message=FALSE}
library(caret)

confusionMatrix(thrones_test_pred$pred, thrones_test_pred$dth_flag) #order is estimate and then truth 
```

And there you have it - you've created your first decision tree model in `R`. 
<br>


# Advantages and disadvantages of decision trees 

The major advantage of using decision trees is that they are intuitively very easy to explain. They closely mirror human decision making, as compared with other regression and classification approaches. They can be displayed graphically, and they can easily handle qualitative predictors without the need to create dummy variables. They are also not sensitive to the **scale** of variables. However, decision trees generally do not have the same level of predictive accuracy as other approaches, since they aren't quite robust. A small change in the data can cause a large change in the final estimated tree.  

<hr> 

# Optional: Random forests

To combat these limitations, the **random forest** algorithm has been developed. You can think of this as being equivalent to a collection of decision trees (generated by bootstrapping your training data) that then each provide a classification outcome for each row of your data. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. The overall classification outcome produced by the random forest for a row involves a form of 'weighted voting' by each tree. 

This is where the idea of a "forest" comes from... a collection of decision trees that have been built which each have their own class prediction. The class with the most votes becomes our model prediction, and this is generally considered more powerful and more stable. For example, if 50 of your trees predict an outcome as `true` and 10 predict an outcome as `false`, the overall prediction of the forest will be `true`. 


## Random forest in R

<br> 

We can create a random forest classifier using the `ranger` package.

```{r}
library(ranger)

rf_classifier <- ranger(dth_flag ~ ., 
                        data = thrones_train, 
                        importance = "impurity", 
                        num.trees = 1000, 
                        mtry = 2, 
                        min.node.size = 5)

rf_classifier
```

<br>

We see the output gives us a summary of the classifier and how it is performing on the training data (the 'OOB prediction error', where 'OOB' stands for 'out-of-bag', i.e. how the classifier performs on data it has not seen). In this case we have three so-called **'hyperparameters'**: `num.trees` (number of trees to construct), `mtry` (number of variables to split at in each node) and `min.node.size` (minimal node size) that affect how the classifier is constructed (5 is regression, 1 is classification, see help doc for more options).  


Ideally we would like to cut a third set of data from the original dataset: the so-called **'validation'** set to 'tune' these hyperparameters (and potentially others) for best performance (or use cross validation). We won't do that at present, however, as the Game of Thrones dataset is quite small, and we're already struggling to provide sufficiently large training and testing sets.   

Because the random forest builds multiple decision trees (in this case each involving `mtry = 2` variables), we can get a measure of how important each variable is to the final performance of the classifier (this is what argument `importance = "impurity"`) did above.

<br> 

```{r}
importance(rf_classifier)
```

<br>

So we see that `prominence` is most important, followed by `allegiance_last`, and then `featured_episode_count`, and so forth...

Alas, `ranger` doesn't work with the `add_predictions()` function, but we just need a small bit of `dplyr` 

<br>

```{r}
thrones_test_pred <- thrones_test %>%
  mutate(pred = predict(rf_classifier, data = thrones_test)$predictions)
```

<br>

and then, finally, calculate the confusion matrix using the function from `caret`. Hopefully you see that performance has been increased relative to the single decision tree. We might hope that hyperparameter tuning using cross-validation or a separate validation set might further improve performance!

<br>

```{r}
confusionMatrix(thrones_test_pred$pred, thrones_test_pred$dth_flag)
```


<hr>

# Recap

* Is it necessary to scale data before generating a decision tree? 
<details>
<summary>**Answer**</summary>
No, we only need to remove any `NA` values
</details>
* What should be the total value of the percentages shown on a decision tree's leaf nodes?
<details>
<summary>**Answer**</summary>
100, although it may be off by 1 due to rounding errors
</details>


# Additional Resources

* [Decision trees in R](https://www.datacamp.com/community/tutorials/decision-trees-R)  

* [Interpreting rpart decision tree plots](http://www.milbo.org/doc/prp.pdf)